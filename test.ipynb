{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725894b4-fc80-4491-838f-bf11c5e5df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.download_pdfs_from_url import download_papers\n",
    "from utils import icml_parser \n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf76bd-e24a-4f82-9d77-0556491f6532",
   "metadata": {},
   "source": [
    "## Project Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b164a3-2f5c-473f-b8dd-a43cba40414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = './examples/icml_2024'\n",
    "\n",
    "paper_pdf_dir = Path(PROJECT_DIR, 'paper_pdfs')\n",
    "paper_parsed_dir = Path(PROJECT_DIR, 'paper_parsed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c646de-437b-4e2e-82ff-548fd17fbd68",
   "metadata": {},
   "source": [
    "## Download and Parse all ICML papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1604065-6c5e-478c-b35e-f6bb6fbdc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = download_papers('https://proceedings.mlr.press/v235/', paper_pdf_dir)\n",
    "\n",
    "all_papers = icml_parser.parse_folder(paper_pdf_dir)\n",
    "with open(paper_parsed_dir, 'wb') as f:\n",
    "    pickle.dump(all_papers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a2064-ca55-4b4a-8157-c74184942410",
   "metadata": {},
   "source": [
    "## Load ICML papers fomr pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bdfc84-5e5e-4869-8a1e-1ce775f68cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(paper_parsed_dir, 'rb') as f:\n",
    "    all_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92d802d-7cfe-4f94-a1bb-980616233a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2610"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b2868-2c21-40de-849d-3bda08a6e710",
   "metadata": {},
   "source": [
    "## Test single paper summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcead795-530a-43cb-bd5e-239b8910c134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoyunfeng/Documents/AI_projects/code/arcknow-local/utils/paper_ontology.py:203: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  '''\n"
     ]
    }
   ],
   "source": [
    "# Google AI Studio parameters \n",
    "from utils.paper_ontology import *\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.configure(api_key='<>')\n",
    "flash = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe5b3a4a-1a3a-4359-98fe-b5f0b03dcba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Genie: Generative Interactive Environments – An Overview for Business Stakeholders\n",
       "\n",
       "This paper introduces Genie, a groundbreaking AI model that generates and allows interaction within virtual worlds.  Instead of simply creating static images or videos, Genie creates fully interactive environments controllable by the user, opening up exciting new possibilities for several businesses.\n",
       "\n",
       "**Problem Statement:**\n",
       "\n",
       "Current generative AI excels at producing individual images or videos, but lacks the ability to create truly interactive and engaging experiences like those found in video games.  Existing methods for creating interactive virtual environments are either labor-intensive (requiring manual design and programming) or limited in scope (only able to generate variations of pre-defined environments).  Genie addresses this gap by aiming to create rich, interactive environments directly from simple user prompts.\n",
       "\n",
       "**Use Cases Impacted:**\n",
       "\n",
       "Genie's potential impacts several business sectors:\n",
       "\n",
       "* **Video Game Development:** Genie can drastically reduce the time and cost of creating new game worlds. Designers could quickly prototype environments from text descriptions or sketches, accelerating the development process.\n",
       "* **Entertainment and Media:**  Interactive storytelling and virtual experiences could be revolutionized.  Imagine creating personalized interactive movies or immersive virtual tours based on simple prompts.\n",
       "* **Training and Simulation:**  Genie could generate realistic simulations for training purposes, such as flight simulators, surgical training, or military exercises.  The ability to control the environment offers a valuable advantage.\n",
       "* **Marketing and Advertising:**  Interactive advertisements and product demonstrations could be created, offering a more engaging and memorable experience for customers.\n",
       "\n",
       "\n",
       "**Proposed Approach:**\n",
       "\n",
       "Genie's key innovation lies in its ability to learn to generate interactive environments from *unlabeled* internet videos.  Unlike previous methods which require meticulously labeled data specifying actions and their effects, Genie learns these relationships automatically.  This unsupervised learning is a significant advantage, reducing the need for expensive data annotation.\n",
       "\n",
       "**Fundamental Techniques:**\n",
       "\n",
       "Genie leverages several key techniques:\n",
       "\n",
       "* **Unsupervised Learning:**  Genie learns from a massive dataset of unlabeled gaming videos, eliminating the need for manual labeling of actions.\n",
       "* **Spatiotemporal Transformers:**  These advanced neural networks allow Genie to process and understand the temporal relationships within video data, essential for generating dynamic interactions.\n",
       "* **Latent Action Model:**  Genie learns a \"latent action space,\" effectively a set of hidden controls that users can employ to interact with the generated environment.  These actions are inferred directly from the video data, again eliminating the need for manual annotation.\n",
       "* **Autoregressive Prediction:** The model predicts the next frame in a video sequence based on previous frames and the user's chosen action.\n",
       "\n",
       "**Existing Methods Used:**\n",
       "\n",
       "Genie utilizes existing techniques like:\n",
       "\n",
       "* **VQ-VAE (Vector Quantized Variational Autoencoder):** Used for compressing video frames into discrete tokens, making processing more efficient.\n",
       "* **MaskGIT:** A transformer-based approach employed for video generation, allowing for the controlled generation of frames.\n",
       "* **Vision Transformers (ViT):** A fundamental building block of the model's architecture for processing image data.\n",
       "\n",
       "**Benchmarks and Metrics:**\n",
       "\n",
       "The researchers tested Genie on two datasets: one comprised of 2D platformer game videos and another containing robot manipulation videos.  They reported:\n",
       "\n",
       "* **Frechet Video Distance (FVD):** Measures the quality of generated videos.  Lower FVD indicates higher quality.\n",
       "* **ΔtPSNR (Delta Peak Signal-to-Noise Ratio):** Measures the controllability of the environment, evaluating how much the generated video changes based on user actions.  Higher ΔtPSNR suggests better controllability.\n",
       "\n",
       "Genie outperformed existing approaches in terms of both video fidelity (FVD) and controllability (ΔtPSNR) by significantly reducing the error rates in generating frames accurately, especially when operating in unsupervised mode. It also showed robustness against inputs that deviate significantly from its training data.\n",
       "\n",
       "\n",
       "**Main Conclusion and Impact:**\n",
       "\n",
       "Genie demonstrates the feasibility of generating interactive environments directly from unlabeled video data, opening new avenues for interactive content creation. It outperforms existing methods by efficiently learning from unlabeled video and maintaining controllability. Its unsupervised nature dramatically reduces data annotation costs.  This could revolutionize various industries, as mentioned above, by allowing for faster, cheaper, and more creative content generation.  Future research will focus on improving the model's efficiency, expanding its capabilities, and exploring applications in agent training and other fields.  For businesses, Genie represents a powerful new tool capable of transforming content creation and simulation.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test single paper summary\n",
    "po = PaperOntology(all_papers[1297], flash)\n",
    "ps = po.create_summary()\n",
    "Markdown(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce151031-5448-43d5-863a-b3be5ed05057",
   "metadata": {},
   "source": [
    "## Test Single Ontology Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755e752-89c6-4c4c-84a0-73dcd2c5c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neo4j database installation see https://neo4j.com/docs/python-manual/current/install/\n",
    "#And comments in paper_ontology.py\n",
    "\n",
    "URI = \"neo4j://localhost:7687\"\n",
    "AUTH = (\"neo4j\", \"secretgraph\")\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n",
    "    print(\"Connection established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bee3189-5cc9-4ddf-a21f-c94091dd533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test single ontology creation\n",
    "pj = po.create_ontology_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7c729ee-fa77-471f-b7be-92eee9fc1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = OntologyKG(URI, AUTH[0], AUTH[1])\n",
    "\n",
    "kg.clean()\n",
    "kg.insert(pj)\n",
    "\n",
    "# Go to http://localhost:7474 and click * to check the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36b946d-716b-43ae-a710-db0d07bd3b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Model::VideoPoet\n",
      "---->HAS_ARCHITECTURE::Architecture::Decoder-only transformer\n",
      "---->PROCESSES::Modality::Images\n",
      "---->PROCESSES::Modality::Videos\n",
      "---->PROCESSES::Modality::Text\n",
      "---->PROCESSES::Modality::Audio\n",
      "---->IS_A::Model::Large Language Model (LLM)\n",
      "---->UNDERGOES::Training Stage::Pretraining\n",
      "---->---->USES::Objective::Multimodal generative objectives\n",
      "---->UNDERGOES::Training Stage::Task-specific adaptation\n",
      "---->PERFORMS::Task::Zero-shot video generation\n",
      "---->PERFORMS::Task::Text-to-video\n",
      "---->PERFORMS::Task::Image-to-video\n",
      "---->PERFORMS::Task::Video editing\n",
      "---->PERFORMS::Task::Video-to-video stylization\n",
      "---->USES::Tokenizer::MAGVIT-v2\n",
      "---->USES::Tokenizer::SoundStream\n",
      "---->HAS_MODULE::Module::Super-resolution module\n",
      "---->EVALUATED_ON::Dataset::MSR-VTT\n",
      "---->EVALUATED_ON::Dataset::UCF-101\n",
      "---->EVALUATED_ON::Dataset::Kinetics 600 (K600)\n",
      "---->EVALUATED_ON::Dataset::Something-Something V2 (SSv2)\n",
      "---->EVALUATED_WITH::Metric::Fr´echet Video Distance (FVD)\n",
      "---->EVALUATED_WITH::Metric::CLIP similarity score\n",
      "---->EVALUATED_WITH::Metric::Inception Score (IS)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "po.create_ontology_str()\n",
    "print(po.ontology_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eaa0cb-f398-41f9-89ab-bb1bd3640143",
   "metadata": {},
   "source": [
    "## Test QA Engine using neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3edff6-01f2-417b-88ca-c7cff9bd574e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RAG Demo https://streamlit-aicamp-1069753422075.us-central1.run.app\n",
    "from utils.paper_QA import *\n",
    "\n",
    "#kg.clean()\n",
    "\n",
    "docs = paper2doc([all_papers[0]])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed6c36b-848c-4be3-a306-60523416f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = EmbeddingDB(\"neo4j://localhost:7687\", \"neo4j\", \"secretgraph\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4e3ecf-dace-4264-87ad-f854e9d312e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.9593460559844971\n",
      "Creation of nanomaterials with specific morphol-\n",
      "ogy remains a complex experimental process,\n",
      "even though there is a growing demand for these\n",
      "materials in various industry sectors. This study\n",
      "explores the potential of AI to predict the morphol-\n",
      "ogy of nanoparticles within the data availability\n",
      "constraints. For that, we first generated a new\n",
      "multi-modal dataset that is double the size of anal-\n",
      "ogous studies. Then, we systematically evaluated\n",
      "performance of classical machine learning and\n",
      "large language models in prediction of nanoma-\n",
      "terial shapes and sizes. Finally, we prototyped\n",
      "a text-to-image system, discussed the obtained\n",
      "empirical results, as well as the limitations and\n",
      "promises of existing approaches.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.9230751991271973\n",
      "Over the past 10 years, there have been several works pre-\n",
      "dicting morphological properties of nanoparticles. However,\n",
      "the majority of them focused on size prediction considering\n",
      "a single experimental system, where the resulting particles\n",
      "conform to the same shape and their sizes can be easily\n",
      "standardized. Some particular examples include size predic-\n",
      "tion for silver nanoparticles ( Chen et al. ,  2016 ;  Shafaei &\n",
      "Khayati ,  2020 ), carbon nanotubes ( Iakovlev et al. ,  2019 ),\n",
      "agar nanospheres ( Zaki et al. ,  2015 ), chitosan nanoparti-\n",
      "cles ( Baharifar & Amani ,  2017 ), polymeric nanoparticles\n",
      "( Shahsavari et al. ,  2013 ;  Soliman et al. ,  2014 ;  Youshia et al. ,\n",
      "2017 ), TiO 2  nanoparticles ( Pellegrino et al. ,  2020 ) and dif-\n",
      "ferent methacrylates ( Kimmig et al. ,  2021 ). In our work,\n",
      "there is no attachment to nanoparticles of a certain shape.\n",
      "Instead, we generate a dataset containing multiple different\n",
      "shapes, which greatly expands the generalizability of our\n",
      "approach and enables future transfer learning applications.\n",
      "In addition, unlike many previous studies, we provide the\n",
      "data for benchmarking and the code for reproducibility.\n",
      "A few published works specialize in predicting the shapes\n",
      "of nanoparticles ( Timoshenko et al. ,  2017 ;  Chen et al. ,  2020 ;\n",
      "Yao et al. ,  2022 ), but they too have certain shortcomings. For\n",
      "example, Timoshenko et al. created a model that takes ex-\n",
      "perimental X-ray absorption near-edge structure (XANES)\n",
      "spectroscopy data as input to predict the 3D structure of\n",
      "metallic nanoparticles ( Timoshenko et al. ,  2017 ). Although\n",
      "circumventing the need for SEM analysis, this approach\n",
      "still requires actual synthesis and experimental evaluation\n",
      "of other properties to predict the shape of the nanomate-\n",
      "rial. This narrows down the list of possible applications\n",
      "significantly. In contrast, our work explores data-driven\n",
      "approaches that only use features of the past syntheses to\n",
      "predict morphology of potentially new nanomaterials.\n",
      "More advanced deep learning algorithms have also found\n",
      "applications in the creation of new nanomaterials ( Roccapri-\n",
      "ore et al. ,  2021 ;  Xu et al. ,  2023 ). In the paper by Kim, Han,\n",
      "and Han, a model based on convolutional neural networks\n",
      "was proposed capable of determining the morphology of\n",
      "nanomaterials based on the SEM images ( Kim et al. ,  2020 ).\n",
      "Such efforts help to better understand morphological prop-\n",
      "erties of nanomaterials and simplify data labeling for the\n",
      "future predictive approaches. However, they do not avoid\n",
      "tedious experimental work preparing the datasets of SEM\n",
      "images, by design. Ultimately, our work stands out by pre-\n",
      "dicting SEM images of nanoparticles of different morpholo-\n",
      "gies based on the properties of the corresponding syntheses,\n",
      "which is an inverse problem formulation.\n",
      "Recent advances in natural language processing ( OpenAI\n",
      "et al. ,  2023 ;  Jiang et al. ,  2023 ;  Touvron et al. ,  2023 ) have\n",
      "also been reflected in some areas of chemistry. Recently,\n",
      "there have been studies that describe the use of LLMs, in\n",
      "particular using the few-shot method, to predict the char-\n",
      "acteristics of various chemical objects ( Zheng et al. ,  2023 )\n",
      "and even to generate new chemical structures ( Jablonka\n",
      "et al. ,  2022 ). However, the potential application of LLMs to\n",
      "predict the morphology of nanomaterials has not yet been\n",
      "investigated.\n",
      "Various multimodal systems have been proposed recently\n",
      "in application to nanomaterial science ( Kononova et al. ,\n",
      "2019 ;  Lee et al. ,  2020 ;  Hiszpanski et al. ,  2020 ). Since the\n",
      "emergence of Stable Diffusion ( Rombach et al. ,  2021 ) and\n",
      "DALL-E ( Ramesh et al. ,  2022 ), image generation models\n",
      "have attracted particularly much public attention. A recent\n",
      "work in nanofabrication presented an image-to-image sys-\n",
      "tem capable of predicting the postfabrication appearance\n",
      "of structures manufactured by focused ion beam milling\n",
      "( Buchnev et al. ,  2022 ). Although a very specialized ap-\n",
      "2\n",
      "plication, it demonstrates how the field of nanotechnology\n",
      "already benefits from generative AI. In this work, we proto-\n",
      "typed a text-to-image solution predicting morphologies of\n",
      "the previously unseen nanomaterials.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = all_papers[0]['sections'][0]['section_content']\n",
    "_ = db.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018eaf0-17e5-4b97-b6d9-ec57c554d6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

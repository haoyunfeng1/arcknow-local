{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725894b4-fc80-4491-838f-bf11c5e5df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.download_pdfs_from_url import download_papers\n",
    "from utils import icml_parser \n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf76bd-e24a-4f82-9d77-0556491f6532",
   "metadata": {},
   "source": [
    "## Project Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b164a3-2f5c-473f-b8dd-a43cba40414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = './examples/icml_2024'\n",
    "\n",
    "paper_pdf_dir = Path(PROJECT_DIR, 'paper_pdfs')\n",
    "paper_parsed_dir = Path(PROJECT_DIR, 'paper_parsed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c646de-437b-4e2e-82ff-548fd17fbd68",
   "metadata": {},
   "source": [
    "## Download and Parse all ICML papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1604065-6c5e-478c-b35e-f6bb6fbdc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = download_papers('https://proceedings.mlr.press/v235/', paper_pdf_dir)\n",
    "\n",
    "all_papers = icml_parser.parse_folder(paper_pdf_dir)\n",
    "with open(paper_parsed_dir, 'wb') as f:\n",
    "    pickle.dump(all_papers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a2064-ca55-4b4a-8157-c74184942410",
   "metadata": {},
   "source": [
    "## Load ICML papers fomr pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bdfc84-5e5e-4869-8a1e-1ce775f68cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(paper_parsed_dir, 'rb') as f:\n",
    "    all_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92d802d-7cfe-4f94-a1bb-980616233a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2610"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b2868-2c21-40de-849d-3bda08a6e710",
   "metadata": {},
   "source": [
    "## Test single paper summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcead795-530a-43cb-bd5e-239b8910c134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoyunfeng/Documents/AI_projects/code/arcknow-local/utils/paper_ontology.py:203: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  '''\n"
     ]
    }
   ],
   "source": [
    "# Google AI Studio parameters \n",
    "from utils.paper_ontology import *\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.configure(api_key='<>')\n",
    "flash = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe5b3a4a-1a3a-4359-98fe-b5f0b03dcba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Overview of VideoPoet: A Large Language Model for Zero-Shot Video Generation**\n",
       "\n",
       "This paper introduces VideoPoet, a groundbreaking model for generating high-quality videos from various inputs like text, images, and audio.  For business stakeholders, this is significant because it opens doors to numerous applications requiring automated video creation, improving efficiency and potentially reducing costs.\n",
       "\n",
       "**Problem Statement:**\n",
       "\n",
       "Current video generation models primarily rely on diffusion-based methods. These methods typically start with a pre-trained image model and then fine-tune it for video, often resulting in a complex and less flexible system.  They also often struggle with generating high-fidelity motion and require significant computational resources. VideoPoet addresses these limitations by leveraging the power and efficiency of Large Language Models (LLMs).\n",
       "\n",
       "**Use Cases Impacted:**\n",
       "\n",
       "VideoPoet's impact spans several business sectors:\n",
       "\n",
       "* **Marketing and Advertising:** Generate marketing videos quickly and easily from text descriptions or existing images, reducing production time and costs.\n",
       "* **Entertainment:** Create short video clips, animations, and even longer videos for social media, games, and other entertainment applications.\n",
       "* **Education and Training:**  Develop engaging educational videos from text scripts or images, simplifying the creation of instructional material.\n",
       "* **E-commerce:** Generate product demonstration videos directly from product descriptions and images, streamlining the creation of online product showcases.\n",
       "* **Film and Animation:** While still in early stages,  the technology shows potential to assist in creating initial drafts or concept videos for larger productions.\n",
       "\n",
       "\n",
       "**Proposed Approach:**\n",
       "\n",
       "VideoPoet utilizes a decoder-only transformer architecture, similar to successful LLMs used for text and code generation.  It's trained in two stages:\n",
       "\n",
       "1. **Pretraining:** VideoPoet learns to generate videos from a diverse set of multimodal inputs and tasks (e.g., text-to-video, image-to-video, video inpainting). This is unlike diffusion models which usually focus on a single task.\n",
       "2. **Task-Specific Adaptation:** The pre-trained model can then be fine-tuned for specific applications to enhance quality or add new capabilities.\n",
       "\n",
       "The key difference from existing diffusion models is the use of a unified LLM architecture. This allows for easier integration of various input modalities and tasks, unlike diffusion models which often require architectural modifications for each new task.\n",
       "\n",
       "\n",
       "**Fundamental Techniques:**\n",
       "\n",
       "VideoPoet leverages several fundamental techniques:\n",
       "\n",
       "* **Large Language Model (LLM) Architecture:**  The core of the model is a transformer-based LLM, known for its ability to handle sequential data and learn complex relationships between different modalities.\n",
       "* **Autoregressive Generation:** The model generates videos sequentially, predicting the next frame based on the previous ones and any conditioning signals (text, images, etc.).\n",
       "* **Multimodal Input:**  The model processes images, videos, text, and audio as discrete tokens, allowing for flexible input combinations.\n",
       "\n",
       "\n",
       "**Existing Methods Used:**\n",
       "\n",
       "VideoPoet employs existing methods and frameworks:\n",
       "\n",
       "* **MAGVIT-v2 Tokenizer:**  For efficient encoding of images and videos into discrete tokens.\n",
       "* **SoundStream Tokenizer:** For encoding audio into discrete tokens.\n",
       "* **T5 XL Encoder:**  For processing text input.\n",
       "* **Windowed Local Attention:**  To manage the computational complexity of processing high-resolution videos.\n",
       "* **Alternating Gradient Descent (AGD):**  For efficient multi-task training.\n",
       "\n",
       "\n",
       "**Benchmarks and Metrics:**\n",
       "\n",
       "VideoPoet was evaluated on standard video generation benchmarks (MSR-VTT, UCF-101, Kinetics-600, Something-Something V2) using metrics like Fr√©chet Video Distance (FVD), CLIP similarity score, and Inception Score (IS).  These metrics assess the realism, consistency, and fidelity of the generated videos.\n",
       "\n",
       "**Outperformance:**\n",
       "\n",
       "VideoPoet demonstrates competitive performance and outperforms other methods in certain aspects, particularly in generating videos with complex and realistic motion, as demonstrated in both automated metrics and human evaluations.  It shows comparable or superior performance to state-of-the-art diffusion models on certain metrics.  Human evaluations confirm its strength in generating more interesting and realistic motion compared to other models.\n",
       "\n",
       "\n",
       "**Main Conclusion and Future Impact:**\n",
       "\n",
       "VideoPoet successfully demonstrates the feasibility and advantages of using LLMs for high-quality video generation.  This approach offers improved flexibility, efficiency, and the potential for zero-shot video generation and diverse editing capabilities.  The research opens new avenues for future research in video generation, and for businesses, it promises more efficient and cost-effective methods for video content creation across various industries.  However, ethical considerations regarding misuse (deepfakes etc.) need to be addressed.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test single paper summary\n",
    "po = PaperOntology(all_papers[81], flash)\n",
    "ps = po.create_summary()\n",
    "Markdown(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce151031-5448-43d5-863a-b3be5ed05057",
   "metadata": {},
   "source": [
    "## Test Single Ontology Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755e752-89c6-4c4c-84a0-73dcd2c5c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neo4j database installation see https://neo4j.com/docs/python-manual/current/install/\n",
    "#And comments in paper_ontology.py\n",
    "\n",
    "URI = \"neo4j://localhost:7687\"\n",
    "AUTH = (\"neo4j\", \"secretgraph\")\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n",
    "    print(\"Connection established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bee3189-5cc9-4ddf-a21f-c94091dd533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test single ontology creation\n",
    "pj = po.create_ontology_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7c729ee-fa77-471f-b7be-92eee9fc1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = OntologyKG(URI, AUTH[0], AUTH[1])\n",
    "\n",
    "kg.clean()\n",
    "kg.insert(pj)\n",
    "\n",
    "# Go to http://localhost:7474 and click * to check the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c36b946d-716b-43ae-a710-db0d07bd3b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Model::VideoPoet\n",
      "---->HAS_ARCHITECTURE::Architecture::Decoder-only transformer\n",
      "---->PROCESSES::Modality::Image\n",
      "---->PROCESSES::Modality::Video\n",
      "---->PROCESSES::Modality::Text\n",
      "---->PROCESSES::Modality::Audio\n",
      "---->IS_A::Model::Large Language Model (LLM)\n",
      "---->UNDERGOES::Training Stage::Pretraining\n",
      "---->---->USES::Objective::Multimodal generative objectives\n",
      "---->UNDERGOES::Training Stage::Task-specific adaptation\n",
      "---->PERFORMS::Task::Zero-shot video generation\n",
      "---->PERFORMS::Task::Text-to-video\n",
      "---->PERFORMS::Task::Image-to-video\n",
      "---->PERFORMS::Task::Video editing\n",
      "---->PERFORMS::Task::Video-to-video stylization\n",
      "---->USES::Tokenizer::MAGVIT-v2\n",
      "---->USES::Tokenizer::SoundStream\n",
      "---->USES::Embedding::T5 XL\n",
      "---->HAS_MODULE::Module::Super-resolution module\n",
      "---->EVALUATED_ON::Dataset::MSR-VTT\n",
      "---->EVALUATED_ON::Dataset::UCF-101\n",
      "---->EVALUATED_ON::Dataset::Kinetics 600 (K600)\n",
      "---->EVALUATED_ON::Dataset::Something-Something V2 (SSv2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "po.create_ontology_str()\n",
    "print(po.ontology_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eaa0cb-f398-41f9-89ab-bb1bd3640143",
   "metadata": {},
   "source": [
    "## Test QA Engine using neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3edff6-01f2-417b-88ca-c7cff9bd574e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RAG Demo https://streamlit-aicamp-1069753422075.us-central1.run.app\n",
    "from utils.paper_QA import *\n",
    "\n",
    "#kg.clean()\n",
    "\n",
    "docs = paper2doc([all_papers[0]])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed6c36b-848c-4be3-a306-60523416f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = EmbeddingDB(\"neo4j://localhost:7687\", \"neo4j\", \"secretgraph\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4e3ecf-dace-4264-87ad-f854e9d312e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.9593460559844971\n",
      "Creation of nanomaterials with specific morphol-\n",
      "ogy remains a complex experimental process,\n",
      "even though there is a growing demand for these\n",
      "materials in various industry sectors. This study\n",
      "explores the potential of AI to predict the morphol-\n",
      "ogy of nanoparticles within the data availability\n",
      "constraints. For that, we first generated a new\n",
      "multi-modal dataset that is double the size of anal-\n",
      "ogous studies. Then, we systematically evaluated\n",
      "performance of classical machine learning and\n",
      "large language models in prediction of nanoma-\n",
      "terial shapes and sizes. Finally, we prototyped\n",
      "a text-to-image system, discussed the obtained\n",
      "empirical results, as well as the limitations and\n",
      "promises of existing approaches.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.9230751991271973\n",
      "Over the past 10 years, there have been several works pre-\n",
      "dicting morphological properties of nanoparticles. However,\n",
      "the majority of them focused on size prediction considering\n",
      "a single experimental system, where the resulting particles\n",
      "conform to the same shape and their sizes can be easily\n",
      "standardized. Some particular examples include size predic-\n",
      "tion for silver nanoparticles ( Chen et al. ,  2016 ;  Shafaei &\n",
      "Khayati ,  2020 ), carbon nanotubes ( Iakovlev et al. ,  2019 ),\n",
      "agar nanospheres ( Zaki et al. ,  2015 ), chitosan nanoparti-\n",
      "cles ( Baharifar & Amani ,  2017 ), polymeric nanoparticles\n",
      "( Shahsavari et al. ,  2013 ;  Soliman et al. ,  2014 ;  Youshia et al. ,\n",
      "2017 ), TiO 2  nanoparticles ( Pellegrino et al. ,  2020 ) and dif-\n",
      "ferent methacrylates ( Kimmig et al. ,  2021 ). In our work,\n",
      "there is no attachment to nanoparticles of a certain shape.\n",
      "Instead, we generate a dataset containing multiple different\n",
      "shapes, which greatly expands the generalizability of our\n",
      "approach and enables future transfer learning applications.\n",
      "In addition, unlike many previous studies, we provide the\n",
      "data for benchmarking and the code for reproducibility.\n",
      "A few published works specialize in predicting the shapes\n",
      "of nanoparticles ( Timoshenko et al. ,  2017 ;  Chen et al. ,  2020 ;\n",
      "Yao et al. ,  2022 ), but they too have certain shortcomings. For\n",
      "example, Timoshenko et al. created a model that takes ex-\n",
      "perimental X-ray absorption near-edge structure (XANES)\n",
      "spectroscopy data as input to predict the 3D structure of\n",
      "metallic nanoparticles ( Timoshenko et al. ,  2017 ). Although\n",
      "circumventing the need for SEM analysis, this approach\n",
      "still requires actual synthesis and experimental evaluation\n",
      "of other properties to predict the shape of the nanomate-\n",
      "rial. This narrows down the list of possible applications\n",
      "significantly. In contrast, our work explores data-driven\n",
      "approaches that only use features of the past syntheses to\n",
      "predict morphology of potentially new nanomaterials.\n",
      "More advanced deep learning algorithms have also found\n",
      "applications in the creation of new nanomaterials ( Roccapri-\n",
      "ore et al. ,  2021 ;  Xu et al. ,  2023 ). In the paper by Kim, Han,\n",
      "and Han, a model based on convolutional neural networks\n",
      "was proposed capable of determining the morphology of\n",
      "nanomaterials based on the SEM images ( Kim et al. ,  2020 ).\n",
      "Such efforts help to better understand morphological prop-\n",
      "erties of nanomaterials and simplify data labeling for the\n",
      "future predictive approaches. However, they do not avoid\n",
      "tedious experimental work preparing the datasets of SEM\n",
      "images, by design. Ultimately, our work stands out by pre-\n",
      "dicting SEM images of nanoparticles of different morpholo-\n",
      "gies based on the properties of the corresponding syntheses,\n",
      "which is an inverse problem formulation.\n",
      "Recent advances in natural language processing ( OpenAI\n",
      "et al. ,  2023 ;  Jiang et al. ,  2023 ;  Touvron et al. ,  2023 ) have\n",
      "also been reflected in some areas of chemistry. Recently,\n",
      "there have been studies that describe the use of LLMs, in\n",
      "particular using the few-shot method, to predict the char-\n",
      "acteristics of various chemical objects ( Zheng et al. ,  2023 )\n",
      "and even to generate new chemical structures ( Jablonka\n",
      "et al. ,  2022 ). However, the potential application of LLMs to\n",
      "predict the morphology of nanomaterials has not yet been\n",
      "investigated.\n",
      "Various multimodal systems have been proposed recently\n",
      "in application to nanomaterial science ( Kononova et al. ,\n",
      "2019 ;  Lee et al. ,  2020 ;  Hiszpanski et al. ,  2020 ). Since the\n",
      "emergence of Stable Diffusion ( Rombach et al. ,  2021 ) and\n",
      "DALL-E ( Ramesh et al. ,  2022 ), image generation models\n",
      "have attracted particularly much public attention. A recent\n",
      "work in nanofabrication presented an image-to-image sys-\n",
      "tem capable of predicting the postfabrication appearance\n",
      "of structures manufactured by focused ion beam milling\n",
      "( Buchnev et al. ,  2022 ). Although a very specialized ap-\n",
      "2\n",
      "plication, it demonstrates how the field of nanotechnology\n",
      "already benefits from generative AI. In this work, we proto-\n",
      "typed a text-to-image solution predicting morphologies of\n",
      "the previously unseen nanomaterials.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = all_papers[0]['sections'][0]['section_content']\n",
    "_ = db.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018eaf0-17e5-4b97-b6d9-ec57c554d6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

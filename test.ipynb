{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725894b4-fc80-4491-838f-bf11c5e5df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.download_pdfs_from_url import download_papers\n",
    "from utils import icml_parser \n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf76bd-e24a-4f82-9d77-0556491f6532",
   "metadata": {},
   "source": [
    "## Project Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b164a3-2f5c-473f-b8dd-a43cba40414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = './examples/icml_2024'\n",
    "\n",
    "paper_pdf_dir = Path(PROJECT_DIR, 'paper_pdfs')\n",
    "paper_parsed_dir = Path(PROJECT_DIR, 'paper_parsed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c646de-437b-4e2e-82ff-548fd17fbd68",
   "metadata": {},
   "source": [
    "## Download and Parse all ICML papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1604065-6c5e-478c-b35e-f6bb6fbdc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = download_papers('https://proceedings.mlr.press/v235/', paper_pdf_dir)\n",
    "\n",
    "all_papers = icml_parser.parse_folder(paper_pdf_dir)\n",
    "with open(paper_parsed_dir, 'wb') as f:\n",
    "    pickle.dump(all_papers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a2064-ca55-4b4a-8157-c74184942410",
   "metadata": {},
   "source": [
    "## Load ICML papers fomr pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bdfc84-5e5e-4869-8a1e-1ce775f68cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(paper_parsed_dir, 'rb') as f:\n",
    "    all_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92d802d-7cfe-4f94-a1bb-980616233a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2610"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b2868-2c21-40de-849d-3bda08a6e710",
   "metadata": {},
   "source": [
    "## Test single paper summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcead795-530a-43cb-bd5e-239b8910c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google AI Studio parameters \n",
    "from utils.paper_ontology import *\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.configure(api_key='<>')\n",
    "flash = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe5b3a4a-1a3a-4359-98fe-b5f0b03dcba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Overview of VideoPoet: A Large Language Model for Zero-Shot Video Generation\n",
       "\n",
       "**For Business Stakeholders:**\n",
       "\n",
       "This research paper introduces VideoPoet, a groundbreaking AI model capable of generating high-quality videos from various inputs like text, images, and audio.  This has significant implications for several businesses.\n",
       "\n",
       "**1. Problem Statement:**\n",
       "\n",
       "Current video generation methods primarily rely on diffusion models. These models often require extensive fine-tuning for each specific task (e.g., text-to-video, image-to-video) and struggle with generating videos containing complex, high-fidelity motion.  They also lack the flexibility to handle multiple tasks within a single model. VideoPoet addresses these limitations by leveraging the power of Large Language Models (LLMs).\n",
       "\n",
       "**2. Use Cases Impacted:**\n",
       "\n",
       "VideoPoet's capabilities impact a broad range of businesses, including:\n",
       "\n",
       "* **Marketing and Advertising:** Generate engaging video ads quickly and efficiently from text descriptions or existing images.  Imagine creating personalized video ads tailored to specific customer segments without extensive video production costs.\n",
       "* **Film and Entertainment:**  Accelerate the creation of movie trailers, animated sequences, or even entire short films with text-based prompts, reducing production time and costs dramatically.  \n",
       "* **Education and Training:** Create dynamic and engaging educational videos easily.  Imagine generating illustrative videos for complex concepts based on simple text descriptions.\n",
       "* **Gaming:**  Generate in-game cinematics and procedural content, improving the efficiency of game development.\n",
       "* **Social Media:** Create short, high-quality videos for various social media platforms from simple text descriptions.\n",
       "* **Virtual and Augmented Reality:** Generate realistic videos for use in VR/AR applications.\n",
       "\n",
       "\n",
       "**3. Proposed Approach:**\n",
       "\n",
       "VideoPoet uses a decoder-only transformer architecture, similar to successful LLMs used for text and code generation.  It's trained in two stages:\n",
       "\n",
       "* **Pretraining:**  The model learns from a massive dataset of images, videos, and audio, using a mix of multimodal generative tasks. This includes generating videos from text, images, or audio; predicting future video frames; and inpainting/outpainting missing parts of videos. This allows the model to learn a rich understanding of various video elements and their relationships.\n",
       "* **Task-specific adaptation:** After pretraining, the model can be further fine-tuned for specific tasks to improve its performance and address specialized requirements.\n",
       "\n",
       "\n",
       "**How it's different:** Unlike existing diffusion models, VideoPoet uses a unified architecture for various video generation tasks, eliminating the need for extensive retraining for each new application. This significantly reduces development time and cost.\n",
       "\n",
       "\n",
       "**4. Fundamental Techniques:**\n",
       "\n",
       "The approach is based on the fundamental concepts of:\n",
       "\n",
       "* **Large Language Models (LLMs):**  Utilizing the power of LLMs to handle diverse modalities and tasks within a single architecture.\n",
       "* **Transformer Networks:**  Leveraging the efficiency and effectiveness of transformer architectures for processing sequential data like videos.\n",
       "* **Tokenization:** Representing various input modalities (text, images, audio) as discrete tokens for unified processing.\n",
       "* **Autoregressive Generation:**  Generating video frames sequentially, conditioned on the previously generated frames and other input signals.\n",
       "\n",
       "**5. Existing Methods, Algorithms, Frameworks:**\n",
       "\n",
       "VideoPoet utilizes:\n",
       "\n",
       "* **MAGVIT-v2 tokenizer:** For efficient image and video encoding.\n",
       "* **SoundStream tokenizer:** For audio encoding.\n",
       "* **T5 XL encoder:** For text embeddings.\n",
       "* **Custom super-resolution transformer:** To efficiently upscale the generated videos to higher resolutions.\n",
       "* **Alternating Gradient Descent (AGD):**  An optimization technique for efficient multi-task training.\n",
       "\n",
       "\n",
       "**6. Benchmarks and Metrics:**\n",
       "\n",
       "The model was evaluated on several standard video generation benchmarks like MSR-VTT and UCF-101 (text-to-video), Kinetics-600 (frame prediction), and Something-Something V2 (inpainting/outpainting).  Metrics reported include:\n",
       "\n",
       "* **Fréchet Video Distance (FVD):** Measures the perceptual similarity between generated and real videos.\n",
       "* **CLIP Similarity Score:**  Evaluates the semantic similarity between generated video and its corresponding text description.\n",
       "* **Inception Score (IS):**  A measure of the quality and diversity of the generated videos.\n",
       "\n",
       "\n",
       "**7. Outperformance:**\n",
       "\n",
       "VideoPoet outperforms several state-of-the-art methods, particularly in generating videos with high-fidelity motions and in zero-shot settings (meaning it can handle tasks not explicitly seen during training).  Human evaluations also show its competitive performance against leading diffusion-based models.  In particular, it excelled in generating videos with interesting and realistic motion.\n",
       "\n",
       "**8. Main Conclusion and Impact:**\n",
       "\n",
       "VideoPoet demonstrates the significant potential of LLMs in high-quality video generation. Its ability to handle multiple tasks in a unified framework, combined with its strong zero-shot performance, has substantial implications for research and business.  Future research can focus on improving visual fidelity, handling finer details, and further exploring its editing capabilities. For businesses, it promises to significantly streamline video creation workflows, reduce production costs, and enable new creative applications. The ease of generating videos from text descriptions alone opens up numerous opportunities for various industries to adopt this technology for faster, more cost-effective video content creation.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test single paper summary\n",
    "po = PaperOntology(all_papers[81], flash)\n",
    "ps = po.create_summary()\n",
    "Markdown(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce151031-5448-43d5-863a-b3be5ed05057",
   "metadata": {},
   "source": [
    "## Test Single Ontology Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e755e752-89c6-4c4c-84a0-73dcd2c5c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established.\n"
     ]
    }
   ],
   "source": [
    "#neo4j database installation see https://neo4j.com/docs/python-manual/current/install/\n",
    "#And comments in paper_ontology.py\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "URI = \"neo4j://localhost:7687\"\n",
    "AUTH = (\"neo4j\", \"secretgraph\")\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n",
    "    print(\"Connection established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bee3189-5cc9-4ddf-a21f-c94091dd533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test single ontology creation\n",
    "pj = po.create_ontology_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7c729ee-fa77-471f-b7be-92eee9fc1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = OntologyKG(URI, AUTH[0], AUTH[1])\n",
    "\n",
    "kg.clean()\n",
    "kg.insert(pj)\n",
    "\n",
    "# Go to http://localhost:7474 and click * to check the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36b946d-716b-43ae-a710-db0d07bd3b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Model::VideoPoet\n",
      "---->USES::Architecture::Decoder-only transformer architecture\n",
      "---->---->PROCESSES::Modality::Image\n",
      "---->---->PROCESSES::Modality::Video\n",
      "---->---->PROCESSES::Modality::Text\n",
      "---->---->PROCESSES::Modality::Audio\n",
      "---->IS_A::Model::Large Language Model (LLM)\n",
      "---->PERFORMS::Task::Video Generation\n",
      "---->---->INCLUDES::Task::Text-to-video\n",
      "---->---->INCLUDES::Task::Image-to-video\n",
      "---->---->INCLUDES::Task::Video editing\n",
      "---->---->INCLUDES::Task::Video-to-video stylization\n",
      "---->---->USES::Method::Diffusion-based methods\n",
      "---->---->---->USES::Model::Stable Diffusion\n",
      "---->USES::Tokenizer::MAGVIT-v2\n",
      "---->USES::Tokenizer::SoundStream\n",
      "---->USES::Embedding::T5 XL embeddings\n",
      "---->USES::Module::Super-resolution module\n",
      "---->EVALUATED_ON::Dataset::MSR-VTT\n",
      "---->EVALUATED_ON::Dataset::UCF-101\n",
      "---->EVALUATED_ON::Dataset::Kinetics 600 (K600)\n",
      "---->EVALUATED_ON::Dataset::Something-Something V2 (SSv2)\n",
      "---->EVALUATED_WITH::Metric::Fr´echet Video Distance (FVD)\n",
      "---->EVALUATED_WITH::Metric::CLIP similarity score\n",
      "---->EVALUATED_WITH::Metric::Inception Score (IS)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "po.create_ontology_str()\n",
    "print(po.ontology_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eaa0cb-f398-41f9-89ab-bb1bd3640143",
   "metadata": {},
   "source": [
    "## Test QA Engine using neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be3edff6-01f2-417b-88ca-c7cff9bd574e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RAG Demo https://streamlit-aicamp-1069753422075.us-central1.run.app\n",
    "from utils.paper_QA import *\n",
    "\n",
    "#kg.clean()\n",
    "\n",
    "docs = paper2doc([all_papers[0]]) # Please update to test indexing all papers\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ed6c36b-848c-4be3-a306-60523416f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<>\"\n",
    "\n",
    "db = EmbeddingDB(\"neo4j://localhost:7687\", \"neo4j\", \"secretgraph\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c4e3ecf-dace-4264-87ad-f854e9d312e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.6931540966033936\n",
      "Statistical tests proved certain associations between the pa-\n",
      "rameters of syntheses and the morphologies of the resulting\n",
      "nanomaterials. Therefore, we attempted to exploit them in\n",
      "predicting shapes and sizes of nanomaterials using classical\n",
      "machine learning algorithms.\n",
      "In some cases, several nanoparticles of different shapes and\n",
      "sizes were present on the same image, so initial 215 syn-\n",
      "theses produced 314 training examples of nanoparticles of\n",
      "different types. Following the logic of the statistical eval-\n",
      "uation, we formulated a set of binary classification tasks,\n",
      "one for each type of shape or a combination of shape and\n",
      "size. In this formulation, we first trained a separate model\n",
      "to distinguish nanoparticles of each particular shape. Then,\n",
      "we ran multiple predictions for each sample during the infer-\n",
      "ence to establish what shapes of nanoparticles were present\n",
      "on the corresponding image. The same logic applied to\n",
      "combinations of shapes and sizes. Notably, some synthe-\n",
      "ses consistently result in nanoparticles of several different\n",
      "shapes. Our approach allows dealing with such ambiguities\n",
      "without the need to determine the prevailing nanomaterial\n",
      "shape or size.\n",
      "5.1. Classical machine learning\n",
      "5.1.1. T REE - BASED ENSEMBLE MODELS\n",
      "We trained the tree-based models, namely Random Forest\n",
      "(RF) and Gradient Boosted Trees (XGB), to predict 9 cate-\n",
      "gories representing combinations of shapes and sizes and 5\n",
      "categories representing shapes only. Therein, we followed\n",
      "all the good practices in data preprocessing and model selec-\n",
      "tion. A thorough description of the process of development,\n",
      "optimization and evaluation of classical machine learning\n",
      "models is presented in the Appendix  A.2 .\n",
      "5.1.2. R ESULTS\n",
      "The accuracy and the F1 scores of the best models evaluated\n",
      "on the test dataset are presented in  Table 2  and  Table 3 . Each\n",
      "experiment was performed 5 times at different random states,\n",
      "and the mean value and standard deviation were calculated.\n",
      "Based on our results, the shapes of nanoparticles can be\n",
      "predicted reasonably well ( Table 2 ). For every nanomaterial\n",
      "shape, RF performed better than XGB, so only RF metrics\n",
      "are displayed. The average accuracy and F1 score were 0.80\n",
      "and 0.66, respectively. Unsurprisingly, the samples of the\n",
      "least represented categories (namely, flat and amorphous\n",
      "shapes) produced lower F1 scores, which decreased the\n",
      "overall metrics.\n",
      "4\n",
      "Extending the number of categories to include the sizes of\n",
      "nanoparticles as well resulted in superior performance of\n",
      "XGB in most cases ( Table 3 ). The overall average accuracy\n",
      "for the task was 0.77, and the average F1 score – 0.53 .\n",
      "This drop in performance was expected, as the number of\n",
      "samples per category became smaller, increasing the risk\n",
      "of overfitting. Underrepresentation becomes even more ap-\n",
      "parent as well for some classes. Apart from evaluating the\n",
      "models on the test set, which had never been used during\n",
      "training, we also explored feature importances as an addi-\n",
      "tional validation step. In most cases, we observed that the\n",
      "top 5 most important parameters were well in agreement\n",
      "with the statistical tests described in the previous section\n",
      "and presented in  Table 6  of the Appendix  A.1 . An example\n",
      "of feature importance analysis for the Random Forest model\n",
      "predicting whether a nanoparticle belongs to a stick shape\n",
      "is shown on  Figure 4  of the Appendix  A.2 .\n",
      "Thus, we demonstrated the possibility of predicting shapes\n",
      "and sizes of NPs with machine learning models, confirmed\n",
      "by average test accuracy of 0.80 and by feature importance\n",
      "analysis coherent with the statistical evaluation. The trained\n",
      "models can already be used to predict morphological prop-\n",
      "erties of new nanomaterials based on their synthesis pro-\n",
      "cedures. However, with recent advances in large language\n",
      "models, we wondered whether similar prediction perfor-\n",
      "mance can be achieved with state-of-the-art LLMs in a few-\n",
      "shot scenario. That would allow material scientists to use\n",
      "natural language for prediction tasks, bypassing the need to\n",
      "develop and optimize complex machine learning pipelines.\n",
      "In the following section, we describe applications of LLMs\n",
      "to nanomaterial shape and size prediction.\n",
      "5.2. Large language models\n",
      "5.2.1. T EXTS OF SYNTHESIS PROCEDURES\n",
      "A dataset of texts describing synthesis procedures was pre-\n",
      "pared for morphology prediction with LLMs and training\n",
      "the text-to-image model. For that, we created a dozen of se-\n",
      "mantic templates with gaps for particular values of synthesis\n",
      "parameters. We leveraged the publicly available GPT-3.5\n",
      "( Liu et al. ,  2023 ) to generate such templates based on a few\n",
      "examples taken from the scientific articles. Thanks to GPT’s\n",
      "strong ability to paraphrase while maintaining the writing\n",
      "style, we managed to collect texts of synthesis procedures\n",
      "sufficiently different from each other in semantics but iden-\n",
      "tical in contents (i.e., the sequence of actions and the list of\n",
      "relevant parameters). The paraphrasing was applied to make\n",
      "sure our approach is capable of handling real-world data and\n",
      "less prone to overfitting. Each template was validated by\n",
      "the experimental team. Peer review included both checking\n",
      "that the experiment description was accurate to maintain re-\n",
      "producibility and that the constructed templates were close\n",
      "to those commonly found in the scientific publications. We\n",
      "also did a practical evaluation of the generated templates to\n",
      "ensure that they contain all the necessary information about\n",
      "the syntheses. For that, we used BERT to extract features\n",
      "of the filled templates and predict the original synthesis\n",
      "parameters with a perceptron. In most cases, we achieved\n",
      "retrieval accuracy close to 1. We provide two examples of\n",
      "the generated templates in the Appendix  A.3 .\n",
      "5.2.2. F EW - SHOT CLASSIFICATION\n",
      "It is now known that LLMs can achieve quite high per-\n",
      "formance in domain-specific regression and classification\n",
      "tasks, often on par with the other widely accepted methods\n",
      "( Jablonka et al. ,  2022 ). In this study, we investigated appli-\n",
      "cations of LLMs to nanomaterial morphology prediction.\n",
      "5\n",
      "For this purpose, we used a few-shot method, in which we\n",
      "show the model only a few randomly selected samples from\n",
      "our training set and then prompt it to make a prediction for\n",
      "a test sample. In all experiments, we used a special prompt\n",
      "describing the task that the LLM was given. It starts as\n",
      "follows:\n",
      "You are an expert in the synthesis of nanomaterials. You\n",
      "analyze the conditions for obtaining a nanomaterial and pre-\n",
      "dict what particle shapes will be present in the synthesized\n",
      "material. There are five particle shapes: ’Cube’, ’Stick’,\n",
      "’Sphere’, ’Flat’ and ’Amorphous’. A nanomaterial can con-\n",
      "tain particles of different shapes. If you cannot say exactly\n",
      "what it is, list the forms that have the highest probability in\n",
      "those conditions.\n",
      "We then appended several random examples from the train-\n",
      "ing set with the corresponding true labels and a single ex-\n",
      "ample from the test subset to the prompt. While doing so,\n",
      "we varied the number of random examples  N , the sam-\n",
      "pling method and the data format. We used from  N  = 2\n",
      "to  N  = 10  training examples in the prompt. We experi-\n",
      "mented with two sampling strategies:  i)  at least one training\n",
      "example belongs to the same target class as the test sam-\n",
      "ple,  ii)  all training examples belong to the same class as\n",
      "the test sample. The choice of the sampling strategies was\n",
      "based on practical considerations around real experiments.\n",
      "More specifically, strategy  i)  is targeted at characterization\n",
      "of previously unknown shapes, while strategy  ii)  follows the\n",
      "logic of a confirmation experiment, when a researcher only\n",
      "needs to confirm the presence of a particular nanomaterial\n",
      "shape in the synthesis. Finally, we used either of the two\n",
      "formats: textual (described in subsection  5.2.1 ) or tabular.\n",
      "In the tabular format, features of the training examples were\n",
      "concatenated to a string along with their values separated by\n",
      "colon, e.g., ”Ca ion, mM: 44; CO3 ion, mM: 159...”. Finally,\n",
      "the LLM was instructed to produce the list of nanoparticle\n",
      "shapes corresponding to the test synthesis as an answer. A\n",
      "more detailed description of prompts is presented in the\n",
      "Appendix  A.3 .\n",
      "Using\n",
      "the\n",
      "above\n",
      "prompt\n",
      "structure,\n",
      "we\n",
      "applied\n",
      "6\n",
      "state-of-the-art\n",
      "LLMs,\n",
      "including\n",
      "GPT-4-turbo\n",
      "( gpt-4-0125-preview ),\n",
      "GPT-4\n",
      "( gpt-4-0613 )\n",
      "and\n",
      "GPT-3.5-turbo\n",
      "( gpt-3.5-turbo-1106 )\n",
      "from\n",
      "OpenAI ( OpenAI ,  2023 ), as well as the latest versions of\n",
      "Mistral Medium, Small and Tiny from Mistral AI ( Jiang\n",
      "et al. ,  2023 ), to the same classification tasks described\n",
      "earlier.\n",
      "To systematically evaluate performance, we\n",
      "repeated each computational experiment 5 times and\n",
      "calculated mean and standard deviation for the standard\n",
      "classification metrics.\n",
      "5.2.3. R ESULTS\n",
      "Table 4  shows top performance of LLMs predicting shapes\n",
      "of nanomaterials. Strikingly, GPT-4 achieved an even higher\n",
      "average accuracy than tree-based ensemble models. Among\n",
      "the other LLMs, it also demonstrated the smallest standard\n",
      "deviation, which speaks for better consistency. Interestingly,\n",
      "the second best model was Mistral-small. Given that its\n",
      "inference time and pricing are much lower than GPT-4, this\n",
      "model could be a pragmatic choice for practitioners as a\n",
      "balanced cost-quality trade-off. A detailed comparison of\n",
      "the pricing, inference time and rate limits is summarized in\n",
      "the  Table 9  of Appendix  A.4 . In addition, we observed some\n",
      "mysterious drops in performance when predicting spherical\n",
      "shape. More specifically, Mistral-medium and GPT-4-turbo\n",
      "produced the accuracy of 0.38 and 0.44, respectively, which\n",
      "dramatically decreased their average scores, while the other\n",
      "models under identical experimental conditions coped with\n",
      "the problem reasonably well.\n",
      "Analyzing the impact of sampling methods and data formats\n",
      "( Table 5  shows results for one of the GPT-4 experiments),\n",
      "we came to the following conclusions. First, including more\n",
      "examples from the training set belonging to the same class\n",
      "as the test sample definitely benefits the prediction. We\n",
      "observed improvements in accuracy in all related cases. Sec-\n",
      "ond, textual and tabular data formats performed similarly.\n",
      "However, textual format consistently resulted in a 4% in-\n",
      "crease in average accuracy, which was expected due to the\n",
      "nature of LLMs.\n",
      "Finally, the number of training samples in the prompt also\n",
      "correlated with the performance metrics ( Figure 1 ). For all\n",
      "shapes except the cube, we observed an increase in accuracy\n",
      "as more examples from the training set were included for\n",
      "prediction. However, longer prompts are also known to\n",
      "trigger hallucination. On top of that, there is a hard limit\n",
      "on the maximum prompt size for many models. Therefore,\n",
      "for any particular application, one has to seek another trade-\n",
      "off between the number of training samples and the total\n",
      "prompt size. In our case, the performance seemed to reach\n",
      "a plateau with 8 samples (see Appendix  A.4  for more de-\n",
      "tails). The same configuration demonstrated the overall top\n",
      "performance ( Table 4 ).\n",
      "Achieving state-of-the-art performance for nanomaterial\n",
      "morphology prediction with LLMs is very exciting for sev-\n",
      "eral reasons. First, it makes it possible for domain experts\n",
      "and experimentalists to avoid implementing complex data\n",
      "engineering pipelines and optimizing machine learning mod-\n",
      "els, and use natural language to obtain the predictions in-\n",
      "stead. Second, it is obvious from our empirical results ( Ta-\n",
      "ble 4 ) that an ensemble of LLMs would by far outperform\n",
      "the best classical ensemble models. Third, based on our\n",
      "empirical results, LLMs look especially advantageous in\n",
      "classification of underrepresented classes, or in small data\n",
      "6\n",
      "scenarios. In particular, GPT-4 demonstrated a significant\n",
      "increase in accuracy when predicting less represented spher-\n",
      "ical, flat and amorphous nanoparticles ( Table 2 ). Altogether,\n",
      "our results look very promising for the broader adoption of\n",
      "LLMs in the nanomaterial science.\n",
      "5.3. Text-to-image system\n",
      "Prediction of a nanoparticle shape as a categorical vari-\n",
      "able based on the selected set of properties describing the\n",
      "synthesis procedure is inherently subject to information\n",
      "loss. Intuitively, images are much better representations of\n",
      "shapes than any handcrafted categories, and the full text of\n",
      "a nanoparticle synthesis carries more information compared\n",
      "to a set of numerical features extracted from it. Therefore,\n",
      "a text-to-image paradigm previously explored in general-\n",
      "purpose applications ( Rombach et al. ,  2021 ) and other do-\n",
      "mains ( Khwaja et al. ,  2022 ) looks appealing in the context of\n",
      "our problem. In the following, we attempt to prototype such\n",
      "a system to explore its potential despite the hard constraints\n",
      "on the sample size.\n",
      "We break down the text-to-image system into three main\n",
      "components. The first one is the natural language process-\n",
      "ing model converting the text of a synthesis procedure to\n",
      "a vector of numerical features. The second component is\n",
      "the generative model with an encoder-decoder architecture\n",
      "designed to learn representations of images of nanoparticles.\n",
      "Finally, the third component is the “linking” model translat-\n",
      "ing the text representations into the image representations.\n",
      "When combined, the three components make a generative\n",
      "system capable of drawing the morphology of a nanomate-\n",
      "rial based on the description of its synthesis ( Figure 2 ).\n",
      "5.3.1. N ATURAL LANGUAGE PROCESSING MODEL\n",
      "The main requirement for the NLP model used for feature\n",
      "extraction was the ability to retain information about the\n",
      "qualitative and the quantitative features of a synthesis. In\n",
      "order to select the NLP model, we formulated several classi-\n",
      "fication and regression tasks related to the key features of\n",
      "a synthesis procedure. We used the linear evaluation setup\n",
      "7\n",
      "with standard metrics ( Kolesnikov et al. ,  2019 ) to compare\n",
      "several pretrained transformer-based models. We found that\n",
      "the classic BERT model ( Devlin et al. ,  2018 ) achieved per-\n",
      "fect scores in most tasks and, therefore, used BERT as the\n",
      "feature extractor in the text-to-image setup ( Figure 2 ). It\n",
      "also met the requirement of being relatively lightweight,\n",
      "easy to start up and use.\n",
      "5.3.2. A UTOENCODER - BASED GENERATIVE MODEL\n",
      "The most widely spread deep learning model architectures\n",
      "capable of generating images are generative adversarial net-\n",
      "works (GANs) ( Goodfellow et al. ,  2014 ), variational autoen-\n",
      "coders (VAEs) ( Kingma & Welling ,  2013 ), and diffusion\n",
      "models ( Rombach et al. ,  2021 ;  Ramesh et al. ,  2022 ). We\n",
      "opted for a variational autoencoder as a more stable and a\n",
      "more suitable solution for small datasets, given the limited\n",
      "amount of data available for training.\n",
      "The central idea of autoencoders is to learn a compressed\n",
      "representation of the input data while solving a data recon-\n",
      "struction problem. Variational autoencoders also imply a\n",
      "certain probabilistic distribution in the input data, which\n",
      "allows it to generate meaningful outputs by sampling the\n",
      "latent representation after the training is complete ( Kingma\n",
      "& Welling ,  2013 ). In order to plug the VAE into the text-\n",
      "to-image system, we first trained it on the set of SEM im-\n",
      "ages and then froze the decoder part ( Figure 2 ). Refer to\n",
      "Appendix  A.5  and  A.6  for tested VAE architectures and\n",
      "evaluation metrics used.\n",
      "We validated the final VAE model by monitoring training\n",
      "losses and evaluation metrics ( Figure 3 ), analyzing indi-\n",
      "vidual examples of reconstructed images and visualizing\n",
      "the space of learned representations allowing to distinguish\n",
      "different clusters of nanoparticle shapes (Appendix  A.9 ).\n",
      "5.3.3. “L INKING ”  AUTOENCODER MODEL\n",
      "The last component of the proposed text-to-image system is\n",
      "the “linking” neural network learning to map representations\n",
      "of the two modalities. Considering the data limitations and\n",
      "the empirical results described earlier, we refrained from\n",
      "using complex model architectures for this task. Instead,\n",
      "we developed another set of shallow autoencoder networks\n",
      "having from 3 to 8 linear layers. Like in the case of VAE,\n",
      "we optimized hyperparameters for each network, including\n",
      "the dimensionality of the latent space, to achieve the low-\n",
      "est reconstruction MSE (see Appendix  A.7  for details on\n",
      "training and generation phases). The best architecture for\n",
      "the “linking” autoencoder is given in Appendix  A.8 .\n",
      "5.3.4. R ESULTS\n",
      "We observed that our prototype of the text-to-image sys-\n",
      "tem copes best with the generation of cubic nanoparticles,\n",
      "which was expected since the cubic shape was the most\n",
      "represented in the training data. For syntheses of this type\n",
      "of nanomaterials, the generated images were often distinct\n",
      "and well-shaped. It was also easier to grasp the size of cubic\n",
      "nanoparticles compared to other types. In general, however,\n",
      "the size of the dataset was insufficient to generate high qual-\n",
      "ity images directly from text. Several examples of generated\n",
      "images are shown on  Figure 5  of the Appendix  A.8 .\n",
      "Despite the limited applicability of this prototype, we re-\n",
      "alized that repeated image generation based on the same\n",
      "synthesis parameters can provide insights into the poly-\n",
      "dispersity of NPs. Polydispersity is normally defined as\n",
      "PdI  = (   σ\n",
      "8\n",
      "ticle diameter, and a is the mean hydrodynamic radius. We\n",
      "performed 50 generations of amorphous NPs with the same\n",
      "synthesis parameters and observed maximal diameters rang-\n",
      "ing from 30 to 80 pixels. As polydispersity characterization\n",
      "is critical for many applications ( Clayton et al. ,  2016 ), a\n",
      "generative model, such as the proposed prototype, could be\n",
      "instrumental in fast  in silico  screening of NPs by estimating\n",
      "PdI  based on the predicted images.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.6847183704376221\n",
      "Creation of nanomaterials with specific morphol-\n",
      "ogy remains a complex experimental process,\n",
      "even though there is a growing demand for these\n",
      "materials in various industry sectors. This study\n",
      "explores the potential of AI to predict the morphol-\n",
      "ogy of nanoparticles within the data availability\n",
      "constraints. For that, we first generated a new\n",
      "multi-modal dataset that is double the size of anal-\n",
      "ogous studies. Then, we systematically evaluated\n",
      "performance of classical machine learning and\n",
      "large language models in prediction of nanoma-\n",
      "terial shapes and sizes. Finally, we prototyped\n",
      "a text-to-image system, discussed the obtained\n",
      "empirical results, as well as the limitations and\n",
      "promises of existing approaches.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.6836113929748535\n",
      "Over the past 10 years, there have been several works pre-\n",
      "dicting morphological properties of nanoparticles. However,\n",
      "the majority of them focused on size prediction considering\n",
      "a single experimental system, where the resulting particles\n",
      "conform to the same shape and their sizes can be easily\n",
      "standardized. Some particular examples include size predic-\n",
      "tion for silver nanoparticles ( Chen et al. ,  2016 ;  Shafaei &\n",
      "Khayati ,  2020 ), carbon nanotubes ( Iakovlev et al. ,  2019 ),\n",
      "agar nanospheres ( Zaki et al. ,  2015 ), chitosan nanoparti-\n",
      "cles ( Baharifar & Amani ,  2017 ), polymeric nanoparticles\n",
      "( Shahsavari et al. ,  2013 ;  Soliman et al. ,  2014 ;  Youshia et al. ,\n",
      "2017 ), TiO 2  nanoparticles ( Pellegrino et al. ,  2020 ) and dif-\n",
      "ferent methacrylates ( Kimmig et al. ,  2021 ). In our work,\n",
      "there is no attachment to nanoparticles of a certain shape.\n",
      "Instead, we generate a dataset containing multiple different\n",
      "shapes, which greatly expands the generalizability of our\n",
      "approach and enables future transfer learning applications.\n",
      "In addition, unlike many previous studies, we provide the\n",
      "data for benchmarking and the code for reproducibility.\n",
      "A few published works specialize in predicting the shapes\n",
      "of nanoparticles ( Timoshenko et al. ,  2017 ;  Chen et al. ,  2020 ;\n",
      "Yao et al. ,  2022 ), but they too have certain shortcomings. For\n",
      "example, Timoshenko et al. created a model that takes ex-\n",
      "perimental X-ray absorption near-edge structure (XANES)\n",
      "spectroscopy data as input to predict the 3D structure of\n",
      "metallic nanoparticles ( Timoshenko et al. ,  2017 ). Although\n",
      "circumventing the need for SEM analysis, this approach\n",
      "still requires actual synthesis and experimental evaluation\n",
      "of other properties to predict the shape of the nanomate-\n",
      "rial. This narrows down the list of possible applications\n",
      "significantly. In contrast, our work explores data-driven\n",
      "approaches that only use features of the past syntheses to\n",
      "predict morphology of potentially new nanomaterials.\n",
      "More advanced deep learning algorithms have also found\n",
      "applications in the creation of new nanomaterials ( Roccapri-\n",
      "ore et al. ,  2021 ;  Xu et al. ,  2023 ). In the paper by Kim, Han,\n",
      "and Han, a model based on convolutional neural networks\n",
      "was proposed capable of determining the morphology of\n",
      "nanomaterials based on the SEM images ( Kim et al. ,  2020 ).\n",
      "Such efforts help to better understand morphological prop-\n",
      "erties of nanomaterials and simplify data labeling for the\n",
      "future predictive approaches. However, they do not avoid\n",
      "tedious experimental work preparing the datasets of SEM\n",
      "images, by design. Ultimately, our work stands out by pre-\n",
      "dicting SEM images of nanoparticles of different morpholo-\n",
      "gies based on the properties of the corresponding syntheses,\n",
      "which is an inverse problem formulation.\n",
      "Recent advances in natural language processing ( OpenAI\n",
      "et al. ,  2023 ;  Jiang et al. ,  2023 ;  Touvron et al. ,  2023 ) have\n",
      "also been reflected in some areas of chemistry. Recently,\n",
      "there have been studies that describe the use of LLMs, in\n",
      "particular using the few-shot method, to predict the char-\n",
      "acteristics of various chemical objects ( Zheng et al. ,  2023 )\n",
      "and even to generate new chemical structures ( Jablonka\n",
      "et al. ,  2022 ). However, the potential application of LLMs to\n",
      "predict the morphology of nanomaterials has not yet been\n",
      "investigated.\n",
      "Various multimodal systems have been proposed recently\n",
      "in application to nanomaterial science ( Kononova et al. ,\n",
      "2019 ;  Lee et al. ,  2020 ;  Hiszpanski et al. ,  2020 ). Since the\n",
      "emergence of Stable Diffusion ( Rombach et al. ,  2021 ) and\n",
      "DALL-E ( Ramesh et al. ,  2022 ), image generation models\n",
      "have attracted particularly much public attention. A recent\n",
      "work in nanofabrication presented an image-to-image sys-\n",
      "tem capable of predicting the postfabrication appearance\n",
      "of structures manufactured by focused ion beam milling\n",
      "( Buchnev et al. ,  2022 ). Although a very specialized ap-\n",
      "2\n",
      "plication, it demonstrates how the field of nanotechnology\n",
      "already benefits from generative AI. In this work, we proto-\n",
      "typed a text-to-image solution predicting morphologies of\n",
      "the previously unseen nanomaterials.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.6785593032836914\n",
      "Each synthesis in our dataset was described by 10 contin-\n",
      "uous and 3 categorical variables that might be influencing\n",
      "the shapes of nanomaterials in different ways. This section\n",
      "describes statistical evaluation of those features to deter-\n",
      "mine whether they are indeed informative of the geometry\n",
      "of nanomaterials, which served as a basis for downstream\n",
      "AI applications.\n",
      "4.1. Analysis of continuous variables\n",
      "Let ( X 1\n",
      "a synthesis which produces cubic nanoparticles. Let ( X 2\n",
      "X 2\n",
      "of any synthesis which always results in nanoparticles of\n",
      "different shapes. We wondered whether the two samples\n",
      "came from the same population or not. If so, each value of\n",
      "the first sample would have had an equal chance of being\n",
      "larger than each value of the second sample. Therefore, the\n",
      "null hypothesis can be formulated as follows:\n",
      "H 0  :  p ( X 1\n",
      "2\n",
      "In fact, this formulation represents the Mann-Whitney U test\n",
      "( Nachar ,  2008 ). We applied it for each of the real-valued\n",
      "parameters of synthesis and each type of the nanomaterial\n",
      "shapes. We found that formation of stick-shaped nanopar-\n",
      "ticles was dependent on the reaction temperature, synthe-\n",
      "sis time, and polymer mass and/or concentration. Cubic\n",
      "shapes of nanoparticles were also associated with certain\n",
      "temperatures and polymer concentrations, as well as the\n",
      "molar mass of the polymer. We used the Kruskal-Wallis\n",
      "H test ( Kruskal & Wallis ,  1952 ), which is analogous to\n",
      "Mann-Whitney U test but applicable to three and more sam-\n",
      "ple groups, Kolmogorov-Smirnov test ( Smirnov ,  1939 ) and\n",
      "ANOVA ( Marsal ,  1987 ) to corroborate these findings. Here-\n",
      "with, we used the significance level = 0.05 and the Bonfer-\n",
      "roni correction method to account for multiple hypothesis\n",
      "testing.\n",
      "4.2. Analysis of categorical variables\n",
      "To establish relationships between categorical parameters\n",
      "of synthesis procedures and the corresponding shapes of\n",
      "nanomaterials, we composed contingency tables as shown\n",
      "in  Table 1 .\n",
      "According to Fisher,  a  ∼ Hypergeometric ( N, K, n ) ,\n",
      "where  N  =  a  +  b  +  c  +  d  is the population size,  K  =  a  +  b\n",
      "is the number of successes and  n  =  a  +  c  is the number\n",
      "of draws ( Fisher ,  1922 ). Therefore, the probability of this\n",
      "outcome is given by:\n",
      "3\n",
      "p  =\n",
      "\u0012 a  +  b\n",
      "a\n",
      "\u0013 \u0012 c  +  d\n",
      "c\n",
      "\u0013\n",
      "\u0012  n\n",
      "a  +  c\n",
      "\u0013\n",
      "=\n",
      "\u0012 a  +  b\n",
      "b\n",
      "\u0013 \u0012 c  +  d\n",
      "d\n",
      "\u0013\n",
      "\u0012  n\n",
      "b  +  d\n",
      "\u0013\n",
      "=\n",
      "=  ( a  +  b )!( c  +  d )!( a  +  c )!( b  +  d )!\n",
      "a ! b ! c ! d ! n !\n",
      "We computed these probabilities for each combination of\n",
      "nanoparticle shape and polymer/surfactant/solvent involved\n",
      "in the synthesis. Using the same significance level and\n",
      "the correction for multiple hypothesis testing as before, we\n",
      "observed several strong associations: stick-shaped nanopar-\n",
      "ticles with polyethylene glycol (PEG) and polyethylenimine\n",
      "(PEI) polymers; flat nanoparticles with presence of PE-\n",
      "DOT:PSS and polyvinylpyrrolidone (PVP); cubic nanopar-\n",
      "ticles with presence of polyacrylic acid (PAA) and PE-\n",
      "DOT:PSS. We also found strong dependencies of nanoparti-\n",
      "cles’ shapes on the following surfactants: Myristyltrimethy-\n",
      "lammonium bromide and Sodium dodecylsulfate. In the\n",
      "case of amorphous nanoparticles, the presence of Propylene\n",
      "glycol and tert-Butanol solvents was also found significant.\n",
      "Finally, we applied the Chi-squared test ( Magnello ,  2005 )\n",
      "to confirm the aforementioned findings. For more infor-\n",
      "mation on how the statistical tests and the most significant\n",
      "associations between particular synthesis parameters and\n",
      "nanomaterial shapes, see Appendix  A.1 .\n",
      "Notably, many of the parameters of syntheses had no ef-\n",
      "fect on the shapes of nanomaterials, e.g., stirring speed,\n",
      "concentrations of Ca and CO 3  ions, presence of Hexade-\n",
      "cyltrimethylammonium bromide and Triton X-100 surfac-\n",
      "tants, and 1-Hexanol and Methyl alcohol solvents. For the\n",
      "downstream machine learning applications, we excluded\n",
      "those features from the data.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.6778371334075928\n",
      "Nowadays, nanomaterials are spread across many fields of\n",
      "science and industry ( Zebarjadi et al. ,  2011 ;  Liu & Lal ,  2015 ;\n",
      "Kairdolf et al. ,  2017 ;  Shifrina et al. ,  2020 ;  Gao et al. ,  2021 ;\n",
      "Takechi-Haraya et al. ,  2022 ). In each of those fields, for a\n",
      "nanomaterial to be fit for purpose, its size, shape, and other\n",
      "morphological parameters must be precisely controlled, as\n",
      "they directly influence toxicity, catalytic activity and other\n",
      "properties of nanomaterials crucial for applications. Alter-\n",
      "ing these parameters also allows to improve efficiency of\n",
      "drug delivery systems ( Sen Gupta ,  2016 ), catalysts ( Shifrina\n",
      "et al. ,  2020 ), energy storage systems ( Pomerantseva et al. ,\n",
      "2019 ), etc.\n",
      "Typically, creating a nanomaterial with a specific set of\n",
      "properties requires a significant number of experiments\n",
      "ranging from a few repetitive syntheses to a dozen of sub-\n",
      "stantially different synthesis procedures ( Vaidyanathan &\n",
      "Sendhilnathan ,  2008 ;  Sun et al. ,  2021 ). Each synthesis is\n",
      "followed by a specific method of analysis to confirm the\n",
      "experimental outcome. One of the most prominent meth-\n",
      "ods for analyzing nanomaterials is the scanning electron\n",
      "microscopy (SEM) ( Smith & Oatley ,  1955 ). With SEM, it\n",
      "is possible to obtain information about the size and shape\n",
      "of nanoparticles (NPs), as well as the structure of the sur-\n",
      "face, surface flaws and contaminants. Currently, the SEM\n",
      "method is deemed irreplaceable despite being costly and\n",
      "time-consuming ( Singh ,  2016 ). On average, one analysis\n",
      "with SEM can cost up to a few hundred US dollars, leading\n",
      "to vast amounts of resources required to run any large-scale\n",
      "study. While conducting such experiments scientists are\n",
      "most often guided by their experience and intuition acquired\n",
      "in past experiments. This is because the problem of de-\n",
      "termining the morphology of nanomaterials based on the\n",
      "synthesis parameters currently has no theoretical or compu-\n",
      "tational solution, in general. At the same time, syntheses\n",
      "of nanomaterials include too many different interdependent\n",
      "parameters for a person to be able to account for. There-\n",
      "fore, there is a high demand ( AbdelHamid et al. ,  2022 ) for\n",
      "predictive models capable of characterizing the properties\n",
      "of nanomaterials bypassing the need of costly experimental\n",
      "work.\n",
      "Artificial intelligence (AI) offers the most promising set\n",
      "of tools to meet this demand. In fact, classical machine\n",
      "learning (ML) models including artificial neural networks\n",
      "have already been successfully applied to many tasks related\n",
      "to nanomaterial science ( Serov & Vinogradov ,  2022 ;  Chen\n",
      "et al. ,  2023 ;  Banaye Yazdipour et al. ,  2023 ). With recent\n",
      "astonishing advances in deep learning ( Jumper et al. ,  2021 ;\n",
      "Rombach et al. ,  2021 ;  Ramesh et al. ,  2022 ;  OpenAI ,  2023 ;\n",
      "Touvron et al. ,  2023 ;  Jiang et al. ,  2023 ;  Merchant et al. ,\n",
      "2023 ), the potential of AI in the design of nanomaterials\n",
      "seems truly immense. However, one has to possess large\n",
      "volumes of carefully curated data to fully exploit the power\n",
      "of AI. As discussed earlier, accumulating the data appropri-\n",
      "ate for the prediction of nanomaterial morphology has been\n",
      "a major challenge for decades. Within realistic data con-\n",
      "straints, the boundaries of AI in the design of nanomaterials\n",
      "are underexplored.\n",
      "1\n",
      "One of the goals of this work was to showcase possible appli-\n",
      "cations of the most recent advances in machine learning to\n",
      "the design of nanomaterials, and bridge the gap between the\n",
      "experimentalists and the machine learning experts. A huge\n",
      "number of scientific groups are engaged in optimization of\n",
      "various parameters of nanomaterials, in particular morphol-\n",
      "ogy ( Shandilya et al. ,  2022 ;  Jiang et al. ,  2022 ;  Tu et al. ,\n",
      "2022 ;  Kommula et al. ,  2024 ), as it allows new industrial\n",
      "applications or improves the properties of existing materials.\n",
      "The importance of this direction of research is also high-\n",
      "lighted by the recent efforts of the leading AI companies,\n",
      "such as DeepMind ( Merchant et al. ,  2023 ).\n",
      "In this study, we aim to unveil the capabilities and limi-\n",
      "tations of AI in predicting morphology of nanomaterials.\n",
      "For that, we first conduct 215 experimental syntheses of\n",
      "calcium carbonate-based nanomaterials of different shapes\n",
      "and sizes. We carefully document the synthesis procedures\n",
      "with the parameters of experimental conditions, take SEM-\n",
      "images of the resulting nanoparticles, segment and manually\n",
      "annotate them with expert knowledge. We investigate the\n",
      "statistical associations in this multimodal dataset and iden-\n",
      "tify features informative of nanoparticle morphology. We\n",
      "further use these findings to train classical ML models to\n",
      "predict sizes and shapes of nanoparticles and achieve 0.77\n",
      "and 0.80 average accuracy, respectively. For the first time in\n",
      "the field of nanomaterial synthesis, we explore the potential\n",
      "of LLMs for prediction tasks. Using few-shot methods, we\n",
      "utilize state-of-the-art models, such as GPT-4, to predict\n",
      "the shapes of nanomaterials and achieve an impressive 0.81\n",
      "average accuracy. Finally, we augment the available data\n",
      "to prototype a text-to-image system aimed at generating\n",
      "an image of a nanoparticle based on the description of its\n",
      "synthesis procedure. In conclusion, we review the obtained\n",
      "empirical results and discuss the future of AI in the field of\n",
      "nanomaterial design.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main idea?\"\n",
    "r_docs = db.query(query, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7018eaf0-17e5-4b97-b6d9-ec57c554d6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This study explores the use of AI, specifically machine learning and large language models (LLMs), to predict the morphology (shape and size) of nanoparticles.  The researchers created a multimodal dataset of nanoparticle syntheses and corresponding SEM images,  performed statistical analysis to identify relevant synthesis parameters, and then trained and evaluated classical machine learning models and LLMs on this dataset.  Finally, they prototyped a text-to-image system to generate nanoparticle images from synthesis descriptions.  The results demonstrate the potential of AI, particularly LLMs, to accurately predict nanoparticle morphology, offering a faster and less expensive alternative to traditional experimental methods.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = ''\n",
    "for doc, score in r_docs:\n",
    "    context += doc.page_content + '\\n'\n",
    "\n",
    "prompt = f'Answer question based on given context. Question: {query} \\n Context: {context}'\n",
    "answer = flash.generate_content(prompt)\n",
    "\n",
    "Markdown(answer.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d84d0b-d1e0-46dd-80ba-56f983f0a900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

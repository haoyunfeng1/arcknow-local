{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725894b4-fc80-4491-838f-bf11c5e5df31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.download_pdfs_from_url import download_papers\n",
    "from utils import icml_parser \n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf76bd-e24a-4f82-9d77-0556491f6532",
   "metadata": {},
   "source": [
    "## Project Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b164a3-2f5c-473f-b8dd-a43cba40414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = './examples/icml_2024'\n",
    "\n",
    "paper_pdf_dir = Path(PROJECT_DIR, 'paper_pdfs')\n",
    "paper_parsed_dir = Path(PROJECT_DIR, 'paper_parsed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c646de-437b-4e2e-82ff-548fd17fbd68",
   "metadata": {},
   "source": [
    "## Download and Parse all ICML papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1604065-6c5e-478c-b35e-f6bb6fbdc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = download_papers('https://proceedings.mlr.press/v235/', paper_pdf_dir)\n",
    "\n",
    "#all_papers = icml_parser.parse_folder(paper_pdf_dir)\n",
    "#with open(paper_parsed_dir, 'wb') as f:\n",
    "#    pickle.dump(all_papers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51a2064-ca55-4b4a-8157-c74184942410",
   "metadata": {},
   "source": [
    "## Load ICML papers fomr pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bdfc84-5e5e-4869-8a1e-1ce775f68cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(paper_parsed_dir, 'rb') as f:\n",
    "    all_papers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92d802d-7cfe-4f94-a1bb-980616233a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2610"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b2868-2c21-40de-849d-3bda08a6e710",
   "metadata": {},
   "source": [
    "## Test single paper summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcead795-530a-43cb-bd5e-239b8910c134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoyunfeng/Documents/AI_projects/code/arcknow-local/utils/paper_ontology.py:203: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  '''\n"
     ]
    }
   ],
   "source": [
    "# Google AI Studio parameters \n",
    "from utils.paper_ontology import *\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.configure(api_key='<>')\n",
    "flash = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe5b3a4a-1a3a-4359-98fe-b5f0b03dcba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## VideoPoet: A Large Language Model for Zero-Shot Video Generation - An Overview for Business Stakeholders\n",
       "\n",
       "This paper introduces VideoPoet, a groundbreaking AI model capable of generating high-quality videos from various inputs like text, images, and audio.  This has significant implications for several businesses.\n",
       "\n",
       "**1. The Problem:**\n",
       "\n",
       "Current methods for generating videos rely heavily on diffusion models, which are complex, computationally expensive, and often require significant fine-tuning for each specific task (e.g., turning text into video, modifying an existing video).  These models struggle with generating videos featuring high-fidelity, complex motion.  The process is also inflexible, requiring separate models for different video manipulation tasks.\n",
       "\n",
       "\n",
       "**2. Use Cases Impacted:**\n",
       "\n",
       "VideoPoet's capabilities impact a wide array of businesses:\n",
       "\n",
       "* **Marketing and Advertising:** Generate engaging video ads tailored to specific demographics or product features directly from text descriptions.\n",
       "* **Film and Entertainment:** Create realistic and stylized videos, potentially streamlining animation and visual effects processes.\n",
       "* **Education and Training:** Produce instructional videos quickly and easily from text scripts or existing images.\n",
       "* **Gaming:**  Generate dynamic in-game content or cut-scenes more efficiently.\n",
       "* **Virtual and Augmented Reality:** Create immersive and interactive experiences by producing high-quality video content on the fly.\n",
       "\n",
       "\n",
       "**3. Proposed Approach:**\n",
       "\n",
       "Unlike existing diffusion-based models, VideoPoet uses a Large Language Model (LLM) architecture.  Think of it like a highly advanced text prediction model, but instead of predicting words, it predicts video frames and audio. It works by converting all inputs (text, images, audio) into a universal code (tokens) that the LLM can understand and process.  This allows for a unified approach to various video generation tasks, eliminating the need for separate models.\n",
       "\n",
       "**4. Fundamental Techniques:**\n",
       "\n",
       "VideoPoet is based on:\n",
       "\n",
       "* **Large Language Models (LLMs):** The core of the model, leveraging the power of LLMs to handle multiple modalities (text, image, audio) in a unified way.\n",
       "* **Transformer Architecture:** The underlying neural network architecture known for its ability to process sequential data like text and video effectively.\n",
       "* **Autoregressive Generation:** The model predicts future frames based on preceding frames, similar to how we predict the next word in a sentence.\n",
       "\n",
       "\n",
       "**5. Existing Methods Used:**\n",
       "\n",
       "The model utilizes:\n",
       "\n",
       "* **MAGVIT-v2 tokenizer:**  A highly efficient method for converting images and videos into the numerical codes (tokens) that the LLM understands.\n",
       "* **SoundStream tokenizer:**  Converts audio into tokens.\n",
       "* **T5 XL encoder:**  A pre-trained model for converting text into embeddings suitable for the LLM.\n",
       "* **Bidirectional Transformer with Windowed Attention:** A technique to efficiently upscale low-resolution videos to high resolution.\n",
       "\n",
       "\n",
       "**6. Benchmarks and Metrics:**\n",
       "\n",
       "The researchers tested VideoPoet on several established video generation benchmarks (MSR-VTT, UCF-101, Kinetics-600, Something-Something V2), comparing it to state-of-the-art methods.  They used metrics like Fréchet Video Distance (FVD – measures the similarity between generated and real videos), CLIP similarity (measures how well the generated video matches a text description), and Inception Score (measures the diversity and quality of generated videos).\n",
       "\n",
       "\n",
       "**7. Outperformance:**\n",
       "\n",
       "VideoPoet showed competitive or superior performance compared to other leading video generation models, especially in generating videos with complex and realistic motions.  Human evaluations confirmed that VideoPoet often surpasses existing models in terms of motion realism and \"interestingness.\"\n",
       "\n",
       "\n",
       "**8. Main Conclusion and Future Impact:**\n",
       "\n",
       "The paper demonstrates that LLMs can be a highly effective approach to video generation, outperforming traditional diffusion models in several key areas.  This opens up new avenues of research and business opportunities:\n",
       "\n",
       "* **Increased efficiency:**  Generating videos becomes faster and cheaper.\n",
       "* **Enhanced creativity:** Easier and more intuitive video creation tools for businesses.\n",
       "* **New possibilities:** Enables previously impossible video applications.\n",
       "\n",
       "However, ethical considerations surrounding deepfakes and misinformation need to be addressed through measures like digital watermarking.  The research strongly suggests that LLM-based video generation is a promising path forward for many industries.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test single paper summary\n",
    "po = PaperOntology(all_papers[81], flash)\n",
    "ps = po.create_summary()\n",
    "Markdown(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce151031-5448-43d5-863a-b3be5ed05057",
   "metadata": {},
   "source": [
    "## Test Single Ontology Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755e752-89c6-4c4c-84a0-73dcd2c5c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neo4j database installation see https://neo4j.com/docs/python-manual/current/install/\n",
    "#And comments in paper_ontology.py\n",
    "\n",
    "URI = \"neo4j://localhost:7687\"\n",
    "AUTH = (\"neo4j\", \"secretgraph\")\n",
    "with GraphDatabase.driver(URI, auth=AUTH) as driver:\n",
    "    driver.verify_connectivity()\n",
    "    print(\"Connection established.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bee3189-5cc9-4ddf-a21f-c94091dd533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test single ontology creation\n",
    "pj = po.create_ontology_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7c729ee-fa77-471f-b7be-92eee9fc1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = OntologyKG(URI, AUTH[0], AUTH[1])\n",
    "\n",
    "kg.clean()\n",
    "kg.insert(pj)\n",
    "\n",
    "# Go to http://localhost:7474 and click * to check the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36b946d-716b-43ae-a710-db0d07bd3b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::Model::VideoPoet\n",
      "---->HAS_ARCHITECTURE::Architecture::Decoder-only transformer\n",
      "---->PROCESSES::Modality::Images\n",
      "---->PROCESSES::Modality::Videos\n",
      "---->PROCESSES::Modality::Text\n",
      "---->PROCESSES::Modality::Audio\n",
      "---->IS_A::Model::Large Language Model (LLM)\n",
      "---->UNDERGOES::Training Stage::Pretraining\n",
      "---->---->USES::Objective::Multimodal generative objectives\n",
      "---->UNDERGOES::Training Stage::Task-specific adaptation\n",
      "---->PERFORMS::Task::Zero-shot video generation\n",
      "---->PERFORMS::Task::Text-to-video\n",
      "---->PERFORMS::Task::Image-to-video\n",
      "---->PERFORMS::Task::Video editing\n",
      "---->PERFORMS::Task::Video-to-video stylization\n",
      "---->USES::Tokenizer::MAGVIT-v2\n",
      "---->USES::Tokenizer::SoundStream\n",
      "---->HAS_MODULE::Module::Super-resolution module\n",
      "---->EVALUATED_ON::Dataset::MSR-VTT\n",
      "---->EVALUATED_ON::Dataset::UCF-101\n",
      "---->EVALUATED_ON::Dataset::Kinetics 600 (K600)\n",
      "---->EVALUATED_ON::Dataset::Something-Something V2 (SSv2)\n",
      "---->EVALUATED_WITH::Metric::Fr´echet Video Distance (FVD)\n",
      "---->EVALUATED_WITH::Metric::CLIP similarity score\n",
      "---->EVALUATED_WITH::Metric::Inception Score (IS)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "po.create_ontology_str()\n",
    "print(po.ontology_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eaa0cb-f398-41f9-89ab-bb1bd3640143",
   "metadata": {},
   "source": [
    "## Test QA Engine using neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3edff6-01f2-417b-88ca-c7cff9bd574e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RAG Demo https://streamlit-aicamp-1069753422075.us-central1.run.app\n",
    "from utils.paper_QA import *\n",
    "\n",
    "#kg.clean()\n",
    "\n",
    "docs = paper2doc([all_papers[0]])\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed6c36b-848c-4be3-a306-60523416f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = EmbeddingDB(\"neo4j://localhost:7687\", \"neo4j\", \"secretgraph\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4e3ecf-dace-4264-87ad-f854e9d312e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Score:  0.9593460559844971\n",
      "Creation of nanomaterials with specific morphol-\n",
      "ogy remains a complex experimental process,\n",
      "even though there is a growing demand for these\n",
      "materials in various industry sectors. This study\n",
      "explores the potential of AI to predict the morphol-\n",
      "ogy of nanoparticles within the data availability\n",
      "constraints. For that, we first generated a new\n",
      "multi-modal dataset that is double the size of anal-\n",
      "ogous studies. Then, we systematically evaluated\n",
      "performance of classical machine learning and\n",
      "large language models in prediction of nanoma-\n",
      "terial shapes and sizes. Finally, we prototyped\n",
      "a text-to-image system, discussed the obtained\n",
      "empirical results, as well as the limitations and\n",
      "promises of existing approaches.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.9230751991271973\n",
      "Over the past 10 years, there have been several works pre-\n",
      "dicting morphological properties of nanoparticles. However,\n",
      "the majority of them focused on size prediction considering\n",
      "a single experimental system, where the resulting particles\n",
      "conform to the same shape and their sizes can be easily\n",
      "standardized. Some particular examples include size predic-\n",
      "tion for silver nanoparticles ( Chen et al. ,  2016 ;  Shafaei &\n",
      "Khayati ,  2020 ), carbon nanotubes ( Iakovlev et al. ,  2019 ),\n",
      "agar nanospheres ( Zaki et al. ,  2015 ), chitosan nanoparti-\n",
      "cles ( Baharifar & Amani ,  2017 ), polymeric nanoparticles\n",
      "( Shahsavari et al. ,  2013 ;  Soliman et al. ,  2014 ;  Youshia et al. ,\n",
      "2017 ), TiO 2  nanoparticles ( Pellegrino et al. ,  2020 ) and dif-\n",
      "ferent methacrylates ( Kimmig et al. ,  2021 ). In our work,\n",
      "there is no attachment to nanoparticles of a certain shape.\n",
      "Instead, we generate a dataset containing multiple different\n",
      "shapes, which greatly expands the generalizability of our\n",
      "approach and enables future transfer learning applications.\n",
      "In addition, unlike many previous studies, we provide the\n",
      "data for benchmarking and the code for reproducibility.\n",
      "A few published works specialize in predicting the shapes\n",
      "of nanoparticles ( Timoshenko et al. ,  2017 ;  Chen et al. ,  2020 ;\n",
      "Yao et al. ,  2022 ), but they too have certain shortcomings. For\n",
      "example, Timoshenko et al. created a model that takes ex-\n",
      "perimental X-ray absorption near-edge structure (XANES)\n",
      "spectroscopy data as input to predict the 3D structure of\n",
      "metallic nanoparticles ( Timoshenko et al. ,  2017 ). Although\n",
      "circumventing the need for SEM analysis, this approach\n",
      "still requires actual synthesis and experimental evaluation\n",
      "of other properties to predict the shape of the nanomate-\n",
      "rial. This narrows down the list of possible applications\n",
      "significantly. In contrast, our work explores data-driven\n",
      "approaches that only use features of the past syntheses to\n",
      "predict morphology of potentially new nanomaterials.\n",
      "More advanced deep learning algorithms have also found\n",
      "applications in the creation of new nanomaterials ( Roccapri-\n",
      "ore et al. ,  2021 ;  Xu et al. ,  2023 ). In the paper by Kim, Han,\n",
      "and Han, a model based on convolutional neural networks\n",
      "was proposed capable of determining the morphology of\n",
      "nanomaterials based on the SEM images ( Kim et al. ,  2020 ).\n",
      "Such efforts help to better understand morphological prop-\n",
      "erties of nanomaterials and simplify data labeling for the\n",
      "future predictive approaches. However, they do not avoid\n",
      "tedious experimental work preparing the datasets of SEM\n",
      "images, by design. Ultimately, our work stands out by pre-\n",
      "dicting SEM images of nanoparticles of different morpholo-\n",
      "gies based on the properties of the corresponding syntheses,\n",
      "which is an inverse problem formulation.\n",
      "Recent advances in natural language processing ( OpenAI\n",
      "et al. ,  2023 ;  Jiang et al. ,  2023 ;  Touvron et al. ,  2023 ) have\n",
      "also been reflected in some areas of chemistry. Recently,\n",
      "there have been studies that describe the use of LLMs, in\n",
      "particular using the few-shot method, to predict the char-\n",
      "acteristics of various chemical objects ( Zheng et al. ,  2023 )\n",
      "and even to generate new chemical structures ( Jablonka\n",
      "et al. ,  2022 ). However, the potential application of LLMs to\n",
      "predict the morphology of nanomaterials has not yet been\n",
      "investigated.\n",
      "Various multimodal systems have been proposed recently\n",
      "in application to nanomaterial science ( Kononova et al. ,\n",
      "2019 ;  Lee et al. ,  2020 ;  Hiszpanski et al. ,  2020 ). Since the\n",
      "emergence of Stable Diffusion ( Rombach et al. ,  2021 ) and\n",
      "DALL-E ( Ramesh et al. ,  2022 ), image generation models\n",
      "have attracted particularly much public attention. A recent\n",
      "work in nanofabrication presented an image-to-image sys-\n",
      "tem capable of predicting the postfabrication appearance\n",
      "of structures manufactured by focused ion beam milling\n",
      "( Buchnev et al. ,  2022 ). Although a very specialized ap-\n",
      "2\n",
      "plication, it demonstrates how the field of nanotechnology\n",
      "already benefits from generative AI. In this work, we proto-\n",
      "typed a text-to-image solution predicting morphologies of\n",
      "the previously unseen nanomaterials.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = all_papers[0]['sections'][0]['section_content']\n",
    "_ = db.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018eaf0-17e5-4b97-b6d9-ec57c554d6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
